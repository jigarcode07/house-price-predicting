{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d035101-2cae-4ad9-af7b-5a62901365f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 1. Load data\u001b[39;00m\n\u001b[0;32m     14\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# Install lightgbm if not installed\n",
    "# !pip install lightgbm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 1. Load data\n",
    "train = pd.read_csv(r'C:\\Users\\HP\\Downloads\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\HP\\Downloads\\test (1).csv')\n",
    "\n",
    "# Save test IDs for submission file\n",
    "test_ids = test['Id']\n",
    "\n",
    "# 2. Target and features\n",
    "y = train['SalePrice']\n",
    "train.drop(['SalePrice', 'Id'], axis=1, inplace=True)\n",
    "test.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "# 3. Combine train and test for consistent preprocessing\n",
    "all_data = pd.concat([train, test], sort=False).reset_index(drop=True)\n",
    "\n",
    "# 4. Fill missing values (simple but effective)\n",
    "for col in all_data.columns:\n",
    "    if all_data[col].dtype == 'object':\n",
    "        all_data[col] = all_data[col].fillna('Missing')\n",
    "    else:\n",
    "        all_data[col] = all_data[col].fillna(all_data[col].median())\n",
    "\n",
    "# 5. Feature engineering: Create new features\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['Age'] = 2025 - all_data['YearBuilt']  # Use current year or dataset year\n",
    "all_data['RemodAge'] = 2025 - all_data['YearRemodAdd']\n",
    "\n",
    "# Drop columns that might be redundant or leak information if needed\n",
    "# (Optional, you can try with/without)\n",
    "# all_data.drop(['YearBuilt', 'YearRemodAdd'], axis=1, inplace=True)\n",
    "\n",
    "# 6. Split back into train/test\n",
    "X_train = all_data.iloc[:len(y), :]\n",
    "X_test = all_data.iloc[len(y):, :]\n",
    "\n",
    "# 7. Identify categorical and numerical features\n",
    "cat_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# 8. One-hot encode categorical variables using ColumnTransformer and Pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # keep numerical features as is\n",
    ")\n",
    "\n",
    "# 9. Define LightGBM Regressor\n",
    "model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# 10. Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', model)\n",
    "])\n",
    "\n",
    "# 11. Cross-validation (5-fold) with RMSE scoring\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def rmse_cv(model):\n",
    "    rmse = -cross_val_score(model, X_train, y, scoring=\"neg_root_mean_squared_error\", cv=kf)\n",
    "    return rmse\n",
    "\n",
    "scores = rmse_cv(pipeline)\n",
    "print(f\"CV RMSE scores: {scores}\")\n",
    "print(f\"Mean CV RMSE: {scores.mean()}\")\n",
    "\n",
    "# 12. Hyperparameter tuning with RandomizedSearchCV (basic example)\n",
    "param_dist = {\n",
    "    'regressor__n_estimators': [100, 300, 500, 1000],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__num_leaves': [31, 50, 100],\n",
    "    'regressor__max_depth': [-1, 10, 20],\n",
    "    'regressor__min_child_samples': [10, 20, 30],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y)\n",
    "print(f\"Best RMSE: {-random_search.best_score_}\")\n",
    "print(f\"Best params: {random_search.best_params_}\")\n",
    "\n",
    "# 13. Train final model on full train data with best parameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(X_train, y)\n",
    "\n",
    "# 14. Predict on test data\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# 15. Prepare submission file\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'SalePrice': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved as 'submission.csv'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe81bf1-38a6-466d-82bf-7db04adb47e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.8/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3d4dec4-a5a2-4a65-a825-85734e70ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3630\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 196\n",
      "[LightGBM] [Info] Start training from score 181441.541952\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3631\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 199\n",
      "[LightGBM] [Info] Start training from score 179651.292808\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3612\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 181104.263699\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3626\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 197\n",
      "[LightGBM] [Info] Start training from score 181327.004281\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3629\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 194\n",
      "[LightGBM] [Info] Start training from score 181081.876712\n",
      "CV RMSE scores: [30083.53869571 26587.44994948 40567.73592285 27129.67057718\n",
      " 21505.28846943]\n",
      "Mean CV RMSE: 29174.736722927577\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 186\n",
      "[LightGBM] [Info] Start training from score 180921.195890\n",
      "Best RMSE: 28318.04670368384\n",
      "Best params: {'regressor__num_leaves': 31, 'regressor__n_estimators': 100, 'regressor__min_child_samples': 30, 'regressor__max_depth': -1, 'regressor__learning_rate': 0.05}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 186\n",
      "[LightGBM] [Info] Start training from score 180921.195890\n",
      "Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Install lightgbm if not installed\n",
    "# !pip install lightgbm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 1. Load data\n",
    "train = pd.read_csv(r'C:\\Users\\HP\\Downloads\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\HP\\Downloads\\test (1).csv')\n",
    "# Save test IDs for submission file\n",
    "test_ids = test['Id']\n",
    "\n",
    "# 2. Target and features\n",
    "y = train['SalePrice']\n",
    "train.drop(['SalePrice', 'Id'], axis=1, inplace=True)\n",
    "test.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "# 3. Combine train and test for consistent preprocessing\n",
    "all_data = pd.concat([train, test], sort=False).reset_index(drop=True)\n",
    "\n",
    "# 4. Fill missing values (simple but effective)\n",
    "for col in all_data.columns:\n",
    "    if all_data[col].dtype == 'object':\n",
    "        all_data[col] = all_data[col].fillna('Missing')\n",
    "    else:\n",
    "        all_data[col] = all_data[col].fillna(all_data[col].median())\n",
    "\n",
    "# 5. Feature engineering: Create new features\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['Age'] = 2025 - all_data['YearBuilt']  # Use current year or dataset year\n",
    "all_data['RemodAge'] = 2025 - all_data['YearRemodAdd']\n",
    "\n",
    "# Drop columns that might be redundant or leak information if needed\n",
    "# (Optional, you can try with/without)\n",
    "# all_data.drop(['YearBuilt', 'YearRemodAdd'], axis=1, inplace=True)\n",
    "\n",
    "# 6. Split back into train/test\n",
    "X_train = all_data.iloc[:len(y), :]\n",
    "X_test = all_data.iloc[len(y):, :]\n",
    "\n",
    "# 7. Identify categorical and numerical features\n",
    "cat_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# 8. One-hot encode categorical variables using ColumnTransformer and Pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # keep numerical features as is\n",
    ")\n",
    "\n",
    "# 9. Define LightGBM Regressor\n",
    "model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# 10. Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', model)\n",
    "])\n",
    "\n",
    "# 11. Cross-validation (5-fold) with RMSE scoring\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def rmse_cv(model):\n",
    "    rmse = -cross_val_score(model, X_train, y, scoring=\"neg_root_mean_squared_error\", cv=kf)\n",
    "    return rmse\n",
    "\n",
    "scores = rmse_cv(pipeline)\n",
    "print(f\"CV RMSE scores: {scores}\")\n",
    "print(f\"Mean CV RMSE: {scores.mean()}\")\n",
    "\n",
    "# 12. Hyperparameter tuning with RandomizedSearchCV (basic example)\n",
    "param_dist = {\n",
    "    'regressor__n_estimators': [100, 300, 500, 1000],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__num_leaves': [31, 50, 100],\n",
    "    'regressor__max_depth': [-1, 10, 20],\n",
    "    'regressor__min_child_samples': [10, 20, 30],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=kf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y)\n",
    "print(f\"Best RMSE: {-random_search.best_score_}\")\n",
    "print(f\"Best params: {random_search.best_params_}\")\n",
    "\n",
    "# 13. Train final model on full train data with best parameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(X_train, y)\n",
    "\n",
    "# 14. Predict on test data\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# 15. Prepare submission file\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'SalePrice': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved as 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f652f94b-a295-4446-bba7-1c0793888e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\HP\n",
      "Full path to submission.csv: C:\\Users\\HP\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Full path to submission.csv:\", os.path.abspath(\"submission.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff7054-6742-48ef-b9b8-8e16c8e404b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
